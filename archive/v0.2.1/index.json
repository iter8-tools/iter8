[{"uri":"https://iter8.tools/archive/v0.2.1/tutorials/deployments/","title":"Automated canary releases with iter8 on Kubernetes and Istio","tags":[],"description":"","content":"This tutorial shows you how iter8 can be used to perform canary releases by gradually shifting traffic to a canary version of a microservice.\nThis tutorial has 5 parts, which are supposed to be tried in order. Here you will learn:\n how to perform a canary rollout with iter8; how to set different success criteria for iter8 to analyze canary releases and determine success or failure; how to have iter8 immediately stop an experiment as soon as a criterion is not met; how to use your own custom metrics in success criteria for canary analyses; and how iter8 can be used for canary releases of both internal and user-facing services.  The tutorial is based on the Bookinfo sample application that is distributed with Istio. This application comprises 4 microservices, namely, productpage, details, reviews, and ratings, as illustrated here. Please, follow our instructions below to deploy the sample application as part of the tutorial.\nYAML files used in the tutorial All Kubernetes YAML files you will need in this tutorial are in the iter8-controller repository here.\nPart 1: Successful canary release: reviews-v2 to reviews-v3 1. Deploy the Bookinfo application At this point, we assume that you have already followed the instructions to install iter8 on your Kubernetes cluster. The next step is to deploy the sample application we will use for the tutorial.\nFirst, let us create a bookinfo-iter8 namespace configured to enable auto-injection of the Istio sidecar:\nkubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/namespace.yaml Next, let us deploy the Bookinfo application:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-tutorial.yaml You should see the following pods in the bookinfo-iter8 namespace. Make sure the pods\u0026rsquo; status is \u0026ldquo;Running.\u0026rdquo; Also, note that there should be 2 containers in each pod, since the Istio sidecar was injected.\n$ kubectl get pods -n bookinfo-iter8 NAME READY STATUS RESTARTS AGE details-v1-68c7c8666d-m78qx 2/2 Running 0 64s productpage-v1-7979869ff9-fln6g 2/2 Running 0 63s ratings-v1-8558d4458d-rwthl 2/2 Running 0 64s reviews-v2-df64b6df9-ffb42 2/2 Running 0 63s We have deployed \u0026ldquo;version 2\u0026rdquo; of the reviews microservice, and version 1 of all others.\nLet us now expose the edge productpage service by creating an Istio Gateway for it.\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-gateway.yaml You should now see the Istio Gateway and VirtualService for productpage, as below:\n$ kubectl get gateway -n bookinfo-iter8 NAME AGE bookinfo-gateway 22s $ kubectl get vs -n bookinfo-iter8 NAME GATEWAYS HOSTS AGE bookinfo [bookinfo-gateway] [bookinfo.sample.dev] 27s As you can see above, we have associated Bookinfo\u0026rsquo;s edge service with a fake host, namely, bookinfo.sample.dev.\n2. Access the Bookinfo application To access the application, you need to determine the ingress IP and port for the application in your environment. You can do so by following steps 3 and 4 of the Istio instructions here to set the environment variables INGRESS_HOST, INGRESS_PORT, and GATEWAY_URL, which will capture the correct IP address and port for your environment. Once you have done so, you can check if you can access the application with the following command:\ncurl -H \u0026#34;Host: bookinfo.sample.dev\u0026#34; -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; If everything is working, the command above should show 200. Note that the curl command above sets the host header to match the host we associated the VirtualService with (bookinfo.sample.dev). If you want to access the application from your browser, you will need to set this header using a browser\u0026rsquo;s plugin of your choice.\n3. Generate load to the application Let us now generate load to the application, emulating requests coming from users. To do so, we recommend you run the command below on a separate terminal:\nwatch -n 0.1 \u0026#39;curl -H \u0026#34;Host: bookinfo.sample.dev\u0026#34; -Is \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34;\u0026#39; This command will send 10 requests per second to the application. Note that the environment variable GATEWAY_URL must have been set as per step 2 above. Among other things, the command output should show an HTTP code of 200, as below:\nHTTP/1.1 200 OK content-type: text/html; charset=utf-8 content-length: 5719 server: istio-envoy (...) 4. Configure a canary rollout for the reviews service At this point, Bookinfo is using version 2 of the reviews service (reviews-v2). Let us now use iter8 to automate the canary rollout of version 3 of this service (reviews-v3).\nFirst, we need to tell iter8 that we are about to perform this canary rollout. To that end, we create an Experiment configuration specifying the rollout details. In this tutorial, let us use the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v3-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v2 candidate: reviews-v3 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 8 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 200 sampleSize: 5 The configuration above specifies the baseline and candidate versions in terms of Kubernetes deployment names. The rollout is configured to last for 8 iterations (maxIterations) of 30s (interval). At the end of each iteration, if the candidate version meets the specified success criteria, the traffic sent to it will increase by 20 percentage points (trafficStepSize) up to 80% (maxTrafficPercentage). At the end of the last iteration, if the success criteria are met, the candidate version will take over from the baseline.\nIn the example above, we specified only one success criterion. In particular, we stated that the mean latency exhibited by the candidate version should not exceed the threshold of 200 milliseconds. At the end of each iteration, iter8-controller calls iter8-analytics, which in turn analyzes the metrics of interest (in this case, only mean latency) against the corresponding criteria. The number of data points analyzed during an experiment is cumulative, that is, it carries over from iteration to iteration.\nThe next step of this tutorial is to actually create the configuration above. To that end, you can either copy and paste the yaml above to a file and then run kubectl apply -n bookinfo-iter8 -f on it, or you can run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v2_to_reviews-v3.yaml You can verify that the Experiment object has been created as shown below:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Pause TargetsNotFound: Missing Candidate reviews-v2 100 reviews-v3 0 As you can see, iter8 is reporting that 100% of the traffic is sent to the baseline version (reviews-v2) and that the candidate (reviews-v3) is missing. As soon as the controller sees the candidate version, it will start the rollout. Next, let us deploy the candidate version to trigger the canary rollout.\n5. Deploy the canary version and start the rollout As soon as we deploy reviews-v3, iter8-controller will start the rollout. To deploy reviews-v3, you can run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v3.yaml Now, if you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v3:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v2 80 reviews-v3 20 At about every 30s you should see the traffic shift towards reviews-v3 by 20 percentage points.\n6. Check the Grafana dashboard You can also check a Grafana dashboard specific to the Experiment object corresponding to the rollout you are running. The URL to the Grafana dashboard for the experiment is the value of the field grafanaURL under the object\u0026rsquo;s status. One way to get the Grafana URL that you can paste to your browser is through the following command:\nkubectl get experiment reviews-v3-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 By default, the base URL given by iter8 to Grafana is http://localhost:3000. In a typical Istio installation, you can port-forward your Grafana from Kubernetes to your localhost\u0026rsquo;s port 3000 with the following command:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 Below is a screenshot of a portion of the Grafana dashboard showing the request rate and the mean latency for reviews-v2 and reviews-v3, right after the controller ended the experiment.\nNote how the traffic shifted towards the canary during the experiment. You can also see that the canary\u0026rsquo;s mean latency was way below the configured threshold of 200 milliseconds.\nPart 2: High-latency canary release: reviews-v3 to reviews-v4 At this point, you must have completed the part 1 of the tutorial successfully. You can confirm it as follows:\n$ kubectl get experiment reviews-v3-rollout -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 The command above\u0026rsquo;s output shows that reviews-v3 took over from reviews-v2 as part of the canary rollout performed before.\n1. Canary rollout configuration Now, let us set up a canary rollout for reviews-v4, using the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v4-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v3 candidate: reviews-v4 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 200 sampleSize: 5 The configuration above is pretty much the same we used in part 1, except that now the baseline version is reviews-v3 and the candidate is reviews-v4.\nTo create the above Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v3_to_reviews-v4.yaml You can list all Experiment objects like so:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Pause TargetsNotFound: Missing Candidate reviews-v3 100 reviews-v4 0 The output above shows the new object you just created, for which the candidate deployment reviews-v4 is missing. Let us deploy reviews-v4 next so that the rollout can begin.\n2. Deploy reviews-v4 and start the rollout As you have already seen, as soon as we deploy the candidate version, iter8-controller will start the rollout. This time, however, the candidate version (reviews-v4) has a performance issue preventing it from satisfying the success criteria in the experiment object. As a result, iter8 will roll back to the baseline version.\nTo deploy reviews-v4, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v4.yaml Now, if you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v4.\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v3 80 reviews-v4 20 However, unlike the previous rollout, traffic will not shift towards the candidate reviews-v4 because it does not meet the success criteria due to a performance problem. At the end of the experiment, iter8 rolls back to the baseline (reviews-v3), as seen below:\n$ kubectl get experiment reviews-v4-rollout -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 3. Check the Grafana dashboard As before, you can check the Grafana dashboard corresponding to the canary release of reviews-v4. To get the URL to the dashboard specific to this canary release, run the following command:\nkubectl get experiment reviews-v4-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 The dashboard screenshot above shows that the canary version (reviews-v4) consistently exhibits a high latency of 5 seconds, way above the threshold of 200 milliseconds specified in our success criterion, and way above the baseline version\u0026rsquo;s latency.\nPart 3: Error-producing canary release: reviews-v3 to reviews-v5 At this point, you must have completed parts 1 and 2 of the tutorial successfully. You can confirm it as follows:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 The command above\u0026rsquo;s output shows that reviews-v3 took over from reviews-v2 as part of the canary rollout performed before on part 1, and that it continues to be the current version after iter8 had determined that reviews-v4 was unsatisfactory.\n1. Canary rollout configuration Now, let us set up a canary rollout for reviews-v5, using the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v5-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v3 candidate: reviews-v5 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 200 sampleSize: 5 - metricName: iter8_error_rate toleranceType: delta tolerance: 0.02 sampleSize: 10 stopOnFailure: true The configuration above differs from the previous ones as follows. We added a second success criterion on the error-rate metric so that the canary version (reviews-v5) not only must have a mean latency below 200 milliseconds, but it also needs to have an error rate that cannot exceed the baseline error rate by more than 2%. That comparative analysis on a metric is specified as a delta tolerance type. Furthermore, the second success criterion sets the flag stopOnFailure, which means iter8 will roll back to the baseline as soon as the error rate criterion is violated and the minimum number of 10 data points is collected (sampleSize = 10).\nTo create the above Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v3_to_reviews-v5.yaml 2. Deploy reviews-v5 and start the rollout As you already know, as soon as we deploy the candidate version, iter8-controller will start the rollout. This time, the candidate version (reviews-v5) has a bug that causes it to return HTTP errors to its callers. As a result, iter8 will roll back to the baseline version based on the success criterion on the error-rate metric defined above.\nTo deploy reviews-v5, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v5.yaml If you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v5.\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v3 80 reviews-v5 20 Because review-v5 has an issue causing it to return HTTP errors, as per the success criteria we have specified the traffic will not shift towards it. Furthermore, because the error-rate success criteria indicated the need to stop on failure, without waiting for the entire duration of the experiment, iter8 will rollback to reviews-v3 quickly. You should see the following after several seconds:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 3. Check the Grafana dashboard As before, you can check the Grafana dashboard corresponding to the canary release of reviews-v5. To get the URL to the dashboard specific to this canary release, run the following command:\nkubectl get experiment reviews-v5-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 The dashboard screenshots above show that traffic to the canary version (reviews-v5) is quickly interrupted. Also, while the reviews-v5 latency is way below the threshold of 200 milliseconds we defined in the latency success criterion, its error rate is 100%, i.e., it generates errors for every single request it processes. That does not meet the error-rate success criterion we defined, which specified that the canary\u0026rsquo;s error rate must be within 2% of that of the baseline (reviews-v3) version. According to the dashboard, reviews-v3 produced no errors at all.\nPart 4: Using a custom metric At this point, you should have completed parts 1, 2, and 3 of the tutorial successfully. You can confirm it as follows:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 The command above\u0026rsquo;s output shows that reviews-v3 took over from reviews-v2 as part of the canary rollout performed before in part 1, and that it continued to be the current version of the reviews service after iter8 had determined that reviews-v4 was unsatisfactory. Similarly, as we saw in the previous part 3, the experiment to rollout reviews-v5 was aborted because of failure to satisfy the success criteria defined by the user.\nIn this tutorial, we will define a custom metric (one not provided by iter8 out of the box) and use it in the success criteria for a canary release.\nBy default iter8 provides a few metrics which you can see if you type:\n$ kubectl get configmap iter8config-metrics -n iter8 -oyaml In principle, any metric that can be derived from the data you have in your Prometheus database that might be meaningful to you in assessing the health of a service version can be used by iter8. Next, we are going to make iter8 aware of a metric that we will call iter8_90_perc_latency, which measures the 90th percentile latency of a service. In order to make iter8 aware of a new metric we need to add it to the iter8config-metrics config map. For the purposes of this tutorial, we will do so by running the following command:\nkubectl apply -n iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/iter8_metrics_extended.yaml Or, if using a newer version of Istio (1.5 or greater) with telemetry v2:\nkubectl apply -n iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/iter8_metrics_extended_telemetry-v2.yaml Note:  For additional information about how to add a new metric to the existing configuration please see this documentation.\n To verify that the new metric has been added to the configmap, you can check it again:\n$ kubectl get configmap iter8config-metrics -n iter8 -oyaml We will now configure an experiment to use this new metric for a canary release.\n1. Canary rollout configuration Now, let us set up a canary rollout for reviews-v6, using the following Experiment configuration:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: reviews-v6-rollout spec: targetService: name: reviews apiVersion: v1 baseline: reviews-v3 candidate: reviews-v6 trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_90_perc_latency toleranceType: threshold tolerance: 200 sampleSize: 5 The configuration uses the newly extended metric iter8_90_perc_latency. The success criteria asserts that the canary version (reviews-v6) must have the 90th percentile latency below 200 milliseconds.\nTo create the above Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_reviews-v3_to_reviews-v6.yaml As usual, iter8 is waiting for the candidate version to be deployed:\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 reviews-v6-rollout Pause TargetsNotFound: Missing Candidate reviews-v3 100 reviews-v6 0 2. Deploy reviews-v6 and start the rollout As soon as we deploy the candidate version, iter8-controller will start the rollout. This time, the candidate version (reviews-v6) is similar to the earlier reviews-v3 which behaved normally. As a result, iter8 will roll forward to the candidate version based on the success criterion on the newly extended metric defined above.\nTo deploy reviews-v6, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/reviews-v6.yaml If you check the state of the Experiment object corresponding to this rollout, you should see that the rollout is in progress, and that 20% of the traffic is now being sent to reviews-v6.\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 reviews-v6-rollout Progressing IterationUpdate: Iteration 1 Started reviews-v3 80 reviews-v6 20 At about every 30s you should see the traffic shift towards reviews-v6 by 20 percentage points.\nAt the end of the experiment, you will see that all traffic has been shifted to the canary version (reviews-v6)\n$ kubectl get experiments -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE reviews-v3-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v2 0 reviews-v3 100 reviews-v4-rollout Completed ExperimentFailed: Not All Success Criteria Met reviews-v3 100 reviews-v4 0 reviews-v5-rollout Completed ExperimentFailed: Aborted reviews-v3 100 reviews-v5 0 reviews-v6-rollout Completed ExperimentSucceeded: All Success Criteria Were Met reviews-v3 0 reviews-v6 100 3. Check the Grafana dashboard As before, you can check the Grafana dashboard corresponding to the canary release of reviews-v6. To get the URL to the dashboard specific to this canary release, run the following command:\nkubectl get experiment reviews-v6-rollout -o jsonpath=\u0026#39;{.status.grafanaURL}\u0026#39; -n bookinfo-iter8 You can also extend the Grafana Dashboard with the new metric by adding a new panel to the dashboard that looks as follows:\nOther configurations such as title, legend, etc can be varied as per the user\u0026rsquo;s preference.\nPart 5: User-facing Canary release: productpage-v1 to productpage-v2 1. Traffic configuration Consider the case now you want to rollout a new version of productpage deployment productpage-v2 and expose the service outside of the cluster to users through the host productpage.deployment.com. You will need to setup an Istio Gateway:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: productpage-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;productpage.deployment.com\u0026#34; You can run the following command to create the Gateway:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/productpage-gateway.yaml Then emulate traffic flowing from outside of the cluster:\nwatch -x -n 0.1 curl -Is -H \u0026#39;Host: productpage.deployment.com\u0026#39; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; 2. Canary rollout configuration As specified in the targetService section of the following Experiment configuration, we have kubernetes service productpage directing traffic to deployments productpge-v1 and productpage-v2. The entry in hosts tells the controller that traffic will come through \u0026quot;productpage.deployment.com\u0026quot; configured in gateway(istio) paroductpage-gateway:\napiVersion: iter8.tools/v1alpha1 kind: Experiment metadata: name: productpage-v2-rollout spec: targetService: name: productpage baseline: productpage-v1 candidate: productpage-v2 port: 9080 hosts: - name: \u0026#34;productpage.deployment.com\u0026#34; gateway: paroductpage-gateway trafficControl: strategy: check_and_increment interval: 30s trafficStepSize: 20 maxIterations: 6 maxTrafficPercentage: 80 analysis: analyticsService: \u0026#34;http://iter8-analytics:8080\u0026#34; successCriteria: - metricName: iter8_latency toleranceType: threshold tolerance: 3.0 sampleSize: 5 To create this Experiment object, run the following command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/canary_productpage-v1_to_productpage-v2.yaml 3. Deploy productpage-v2 and start the rollout To deploy the candidate version, productpage-v2, run the command:\nkubectl apply -n bookinfo-iter8 -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/productpage-v2.yaml Now check the state of the Experiment object. You should see that the rollout is in progress, and that a portion of the traffic is being sent to productpage-v2.\n$ kubectl get experiment productpage-v2-rollout -n bookinfo-iter8 NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-v2-rollout Progressing IterationUpdate: Iteration 1 Started productpage-v1 80 productpage-v2 20 Cleanup You can cleanup by deleting the namespace:\nkubectl delete ns bookinfo-iter8 "},{"uri":"https://iter8.tools/archive/v0.2.1/","title":"Homepage","tags":[],"description":"","content":"Deliver better software in the cloud Use iter8\u0026rsquo;s analytics-driven continuous experimentation for reliable and frequent releases of high-quality microservices on Kubernetes.\n Automate canary releases Use advanced statistical algorithms to assess key metrics for your service and progressively shift traffic to the winning release.\n Launch experiments rapidly With iter8\u0026rsquo;s Kiali UI, you can create and launch canary release experiments for your service in seconds, and observe and control these experiments in real-time.\n Analyze long-term trends Analyze how key metrics for your service have evolved over multiple releases using iter8-trend and Grafana.\n Explore iter8\n"},{"uri":"https://iter8.tools/archive/v0.2.1/introduction/","title":"Introduction","tags":[],"description":"","content":"Introduction  Iter8 enables statistically robust continuous experimentation of microservices in your CI/CD pipelines  Learn more about iter8\n   Installation  Installation Iter8 on Kubernetes and Istio Learn how to install iter8 on Kubernetes and Istio Iter8 on Red Hat OpenShift Learn how to install iter8 on Red Hat OpenShift   Iter8 on Kubernetes and Istio  Learn how to install iter8 on Kubernetes and Istio\n   Iter8 on Red Hat OpenShift  Learn how to install iter8 on Red Hat OpenShift\n    "},{"uri":"https://iter8.tools/archive/v0.2.1/introduction/about/","title":"Iter8 enables statistically robust continuous experimentation of microservices in your CI/CD pipelines","tags":[],"description":"","content":"Use an iter8 experiment to safely expose competing versions of a service to application traffic, gather in-depth insights about key performance and business metrics for your microservice versions, and intelligently rollout the best version of your service.\nIter8\u0026rsquo;s expressive model of cloud experimentation supports a variety of CI/CD scenarios. Using an iter8 experiment, you can:\n Run a performance test with a single version of a microservice. Perform a canary release with two versions, a baseline and a candidate. Iter8 will shift application traffic safely and gradually to the candidate, if it meets the criteria you specify in the experiment. Perform an A/B test with two versions \u0026ndash; a baseline and a candidate. Iter8 will identify and shift application traffic safely and gradually to the winner, where the winning version is defined by the criteria you specify in the experiment. Perform an A/B/N test with multiple versions \u0026ndash; a baseline and multiple candidates. Iter8 will identify and shift application traffic safely and gradually to the winner.  Under the hood, iter8 uses advanced Bayesian learning techniques coupled with multi-armed bandit approaches to compute a variety of statistical assessments for your microservice versions, and uses them to make robust traffic control and rollout decisions.\n"},{"uri":"https://iter8.tools/archive/v0.2.1/introduction/installation/kubernetes/","title":"Iter8 on Kubernetes and Istio","tags":[],"description":"","content":"These instructions show you how to set up iter8 on Kubernetes with Istio.\nPrerequisites  Kubernetes v1.11 or newer. Istio v1.1.5 and newer. Your Istio installation must have at least the istio-pilot as well as telemetry and Prometheus enabled.  Install iter8 on Kubernetes iter8 has two components, iter8_analytics and iter8_controller. To install them, follow the instructions below. For additional considerations when installing iter8 on Red Hat OpenShift, check out these instructions.\nQuick installation To install iter8 with the default settings, you can run the following install script:\ncurl -L -s https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/install.sh \\ | /bin/bash - Customized installation via Helm charts In case you need to customize the installation of iter8, use the Helm charts listed below:\n  iter8-analytics: https://github.com/iter8-tools/iter8-analytics/releases/download/v0.2.1/iter8-analytics-helm-chart.tar\n  iter8-controller: https://github.com/iter8-tools/iter8-controller/releases/download/v0.2.1/iter8-controller-helm-chart.tar\n  Note on Prometheus: In order to make assessments, iter8-analytics needs to query metrics collected by Istio and stored on Prometheus. The default values for the helm chart parameters (used in the quick installation) point iter8-analytics to the Prometheus server at http://prometheus.istio-system:9090 (the default internal Kubernetes URL of Prometheus installed as an Istio addon) without specifying any need for authentication. If your Istio installation is shipping metrics to a different Prometheus service, or if you need to configure authentication to access Prometheus, you need to set appropriate iter8-analytics Helm chart parameters. Look in the section metricsBackend of the Helm chart\u0026rsquo;s values.yaml file for details.\nNote on Istio Telemetry: When deploying iter8-controller using helm, make sure to set the parameter istioTelemetry to conform with your environment. Possible values are v1 or v2. Use v1 if the Istio mixer is not disabled. You can determine whether or not the mixer is disabled using this command:\nkubectl -n $ISTIO_NAMESPACE get cm istio -o json | jq .data.mesh | grep -o \u0026#39;disableMixerHttpReports: [A-Za-z]\\+\u0026#39; | cut -d \u0026#39; \u0026#39; -f2 Verify the installation After installing iter8-analytics and iter8-controller, you should see the following pods and services in the newly created iter8 namespace:\n$ kubectl get pods -n iter8 NAME READY STATUS RESTARTS AGE iter8-controller-5f54bb4b88-drr8s 1/1 Running 0 4s iter8-analytics-5c5758ccf9-p575b 1/1 Running 0 61s $ kubectl get svc -n iter8 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iter8-controller ClusterIP 172.21.62.217 \u0026lt;none\u0026gt; 443/TCP 20s iter8-analytics ClusterIP 172.21.106.44 \u0026lt;none\u0026gt; 80/TCP 76s Import iter8\u0026rsquo;s Grafana dashboard To enable users to see Prometheus metrics that pertain to their canary releases or A/B tests, iter8 provides a Grafana dashboard template. To take advantage of Grafana, you will need to import this template. To do so, first make sure you can access Grafana. In a typical Istio installation, you can port-forward Grafana from Kubernetes to your localhost\u0026rsquo;s port 3000 with the command below:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 After running that command, you can access Grafana\u0026rsquo;s UI at http://localhost:3000.Iter8 dashboard can be imported by:\ncurl -L -s https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/hack/grafana_install_dashboard.sh \\ | /bin/bash - Uninstall iter8 If you want to uninstall all iter8 components from your Kubernetes cluster, first delete all instances of Experiment from all namespaces. Then, you can delete iter8 by running the following command:\nkubectl delete -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/iter8-controller.yaml Note that this command will delete the Experiment CRD and wipe out the iter8 namespace, but it will not remove the iter8 Grafana dashboard if created.\n"},{"uri":"https://iter8.tools/archive/v0.2.1/introduction/installation/red-hat/","title":"Iter8 on Red Hat OpenShift","tags":[],"description":"","content":"These instructions show you how to set up iter8 on Red Hat OpenShift.\nPrerequisites We recommend using the Red Hat OpenShift Service Mesh. This can be installed using the Red Hat OpenShift Service Mesh Operator. For details, see: https://docs.openshift.com/container-platform/4.3/service_mesh/service_mesh_install/installing-ossm.html.\nInstalling the Service Mesh involves installing the Elasticsearch, Jaeger, Kiali and Red Hat OpenShift Service Mesh Operators, creating and managing a ServiceMeshControlPlane resource to deploy the control plane, and creating a ServiceMeshMemberRoll resource to specify the namespaces associated with the Red Hat OpenShift Service Mesh.\nInstalling iter8 By default, iter8 uses the Prometheus service installed as part of the Red Hat OpenShift Service Mesh for the metrics used to assess the quality of different versions of a service. The Red Hat OpenShift Service Mesh configures the Prometheus service to require authentication. To configure iter8 to authenticate with Prometheus, some additional steps are needed.\nInstall the iter8 analytics service Download and untar the helm chart for the iter8-analytics service. The following options can be used to generate the needed yaml:\nREPO=iter8/iter8-analytics PROMETHEUS_SERVICE=\u0026#39;https://prometheus.istio-system:9090\u0026#39; PROMETHEUS_USERNAME=\u0026#39;internal\u0026#39; PROMETHEUS_PASSWORD=\u0026lt;FILL IN\u0026gt; helm template install/kubernetes/helm/iter8-analytics \\  --name iter8-analytics \\  --set image.repository=${REPO} \\  --set image.tag=v0.2.1 \\  --set iter8Config.authentication.type=basic \\  --set iter8Config.authentication.username=${PROMETHEUS_USERNAME} \\  --set iter8Config.authentication.password=${PROMETHEUS_PASSWORD} \\  --set iter8Config.authentication.insecure_skip_verify=true \\  --set iter8Config.metricsBackendURL=${PROMETHEUS_SERVICE} \\ | kubectl -n iter8 apply -f - The password, to be used can be found in the secret htpasswd in the namespace where Istio is installed. For example, the following might work to identify it:\nPROMETHEUS_PASSWORD=$(kubectl -n istio-system get secret htpasswd -o jsonpath=\u0026#39;{.data.rawPassword}\u0026#39; | base64 --decode) Install the iter8 controller The quick install instructions can be used to install the iter8 controller. The Service Mesh currently uses Istio telemetry version v1:\nkubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/install/iter8-controller.yaml Target Services The Red Hat OpenShift Service Mesh is restricted to the set of namespaces defined in the ServiceMeshMemberRoll resource. In particular, if you will be trying the tutorials, add the namespace bookinfo-iter8 to the ServiceMeshMemberRoll.\nIstio relies a sidecar injected into each pod to provide its capabilities. Istio provides several ways this sidecar can be injected. Red Hat recommends the use of the annotation sidecar.istio.io/inject: \u0026quot;true\u0026quot; in the deployment yaml. Examples can be found in the yaml for the tutorial: https://github.com/iter8-tools/iter8-controller/blob/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-tutorial.yaml\n"},{"uri":"https://iter8.tools/archive/v0.2.1/reference/metrics/","title":"Iter8&#39;s metrics","tags":[],"description":"","content":"This document describes iter8\u0026rsquo;s out-of-the-box metrics, the anatomy of a metric definition, and how users can define their own metrics.\nMetrics defined by iter8 By default, iter8 leverages the metrics collected by Istio telemetry and stored in Prometheus. Users relying on iter8\u0026rsquo;s out-of-the-box metrics can simply reference them in the success criteria of an experiment specification, as shown in the Experiment CRD documentation.\nDuring an experiment, for every call made from iter8-controller to iter8-analytics, the latter in turn calls Prometheus to retrieve values of the metrics referenced by the Kubernetes experiment resource. Iter8-analytics analyzes the service versions that are part of the experiment and arrives at an assessment based on their metric values. It returns this assessment to iter8-controller.\nIn particular, the following metrics are available out-of-the-box from iter8. These metrics are based on the telemetry data collected by Istio.\n  iter8_latency: mean latency, that is, average time taken by the service version to respond to HTTP requests.\n  iter8_error_count: total error count, that is, number of HTTP requests that resulted in error (5xx HTTP status codes).\n  iter8_error_rate: error rate, that is, (total error count / total number of HTTP requests).\n  When iter8 is installed, a Kubernetes ConfigMap named iter8config-metrics is populated with a definition for each of the above metrics. You can see the metric definitions in this file. A few things to note in the definitions:\n  Each metric is defined under the metrics section.\n  They refer back to a Prometheus query template defined under the query_templates section. Iter8 uses that template to query Prometheus and compute the value of the metric for every service version.\n  If this metric is a counter (i.e., its value never decreases over time), then the is_counter key corresponding to this metric is set to True; otherwise, it is set to False.\n  If the value of a metric is unavailable (for example, Prometheus returned NaN or a null value for the query corresponding to his metric), then, by default, iter8 sets the value of this metric to 0. This can be changed to any other float value (specified by the user in string format, e.g., \u0026quot;22.8\u0026quot;) or to \u0026quot;None\u0026quot; using the absent_value key.\n  Finally, each metric is associated with another key sample_size_query_template whose value is a Prometheus query template. Iter8 relies on the notion of a sample-size to compute the total number of data points used in the computation of the metric values. Each of the iter8-defined metrics is associated with the iter8_sample_size query template defined under query_templates, which computes the total number of requests received by a service version. For the default iter8 metrics (mean latency, error count, and error rate), the total number of requests is the correct sample size measure.\n  Adding a new metric Next, we describe how iter8 can be extended with new metrics through the iter8-metrics ConfigMap. Any metric you define in the ConfigMap can then be referenced in the success criteria of experiments.\nAs an example, we will define a new metric called error_count_400s which computes the total count of HTTP requests that resulted in a 400 HTTP status code. Adding a new metric involves creating a Prometheus query template and associating this template with the metric definition. We now describe the structure of a Prometheus query template.\nPrometheus query template A sample query template is shown below:\nsum(increase(istio_requests_total{response_code=~'4..',reporter='source'}[$interval]$offset_str)) by ($entity_labels) As shown above, the query template has three placeholders (i.e., terms beginning with $). These placeholders are substituted with actual values by iter8-analytics in order to construct a Prometheus query. 1) The query template has a group by clause (specified using the by keyword) with the placeholder $entity_labels as the group key. Each group in a Prometheus response corresponds to a distinct entity. Iter8-analytics maps service versions to Prometheus entities using this placeholder. 2) The time period of aggregation is captured by the placeholder $interval. 3) The placeholder $offset_str is used by iter8-analytics to deal with historical data when available. All three placeholders are required in the query template. When a template is instantiated (i.e., placeholders are substituted with values), it results in a Prometheus query expression; when we query Prometheus using this expression, the response from Prometheus needs to be an instant vector.\nUpdating the iter8-metrics ConfigMap There are two steps involved in adding a new metric. Step 1: Extend the query_templates section of the ConfigMap.\nerror_count_400s: \u0026#34;sum(increase(istio_requests_total{job=\u0026#39;istio-mesh\u0026#39;,response_code=~\u0026#39;4..\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels)\u0026#34; For example, we have added our sample query template under a new key called error_count_400s in the query_templates section of the ConfigMap. This query assumes that we are using Istio telemetry v1.\nStep 2: We define a new metric in the metrics section of the ConfigMap as follows:\n- name: error_count_400s is_counter: True sample_size_query_template: iter8_sample_size The interpretation of the above definition is as follows.\n  name: Name of the new metric being defined. Its value is the key for the associated query template. In our example, the relevant key is error_count_400s.\n  is_counter: Set this to True if this metric is a counter metric. In our example, 4xx errors can never decrease over time, and therefore is_counter is set to True.\n  sample_size_query_template: As explained earlier, this is the sample size query template associated with this metric. The sample over which this value is computed for a version is the set of all HTTP requests received by the version. Hence, we are relying on the pre-defined sample-size query template iter8_sample_size, which computes the total number of HTTP requests for a version. If you are defining a metric that requires the sample size to be computed differently, you can create a new sample-size query template (with its own unique name) in the query_templates section and reference it in the metric declaration.\n  "},{"uri":"https://iter8.tools/archive/v0.2.1/reference/","title":"Reference","tags":[],"description":"","content":"Reference  Iter8\u0026#39;s metrics  Iter8\u0026rsquo;s out-of-the-box metrics and how users can define their own metrics\n   Iter8\u0026#39;s experiment CRD  A new Kubernetes CRD that compares baseline and candidate deployments\n   Iter8\u0026#39;s algorithms  The algorithms used to make decisions during canary releases or A/B testing\n   "},{"uri":"https://iter8.tools/archive/v0.2.1/tutorials/","title":"Tutorials","tags":[],"description":"","content":"Tutorials  Automated canary releases with iter8 on Kubernetes and Istio  Perform a canary releases by shifting traffic to a canary version of a microservice.\n   Automated Canary Rollout Using Services  Perform a canary rollout when different versions have different service names\n   "},{"uri":"https://iter8.tools/archive/v0.2.1/tutorials/services/","title":"Automated Canary Rollout Using Services","tags":[],"description":"","content":"In iter8 the versions of a service being compared can be specified using deployment names or using service names. Other tutorials showed how to specify different versions using Kubernetes deployment names. In this tutorial, we learn how to do a canary rollout of an application when different versions are indicated by different Kubernetes service names.\nIn this tutorial, we again consider the user facing service productpage of the bookinfo application and we learn how to create an iter8 Experiment that specifies the baseline and candidate versions using Kubernetes services. The scenario we consider is here:\nIn this example, the application productpage.example.com can be routed, via an Istio Gateway and VirtualService, to the Kubernetes services. Iter8 can be used to automate the rollout including the creation of the Istio VirtualService.\nStep 1: Deploy the bookinfo Application Create a new namespace: $NAMESPACE. We use the name bookinfo-serivce.\nexport NAMESPACE=bookinfo-service kubectl create ns $NAMESPACE kubectl label ns $NAMESPACE istio-injection=enabled Deploy the bookinfo application to a new namespace. In particular, we create the service productpage-v1 to access the productpage application.\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/bookinfo-tutorial.yaml -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/productpage-v1.yaml Step 2: Configure Traffic to the Application Create an Istio gateway for the external host productpage.example.com:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/bookinfo-gateway.yaml At this point, the application is not actually accessible to users because no VirtualService has been defined. Rather than manually define it, this tutorial shows how iter8 can be used to create it for us. To do so, define an iter8 canary experiment from the current version of the application to itself. Clearly, this will succeed, and, as a side effect, a VirtualService will be created.\nIn the experiment, the targetService will look like:\ntargetService: kind: Service baseline: productpage-v1 candidate: productpage-v1 hosts: - name: productpage.example.com gateway: productpage-service It identifies the type of the baseline and candidate as services using kind: Service. The baseline and candidate names are the same. Further, it identifies the external host name and the Istio Gateway already configured.\nTo optimize the bootstrapping process, we can eliminate all of the successCriteria. This has the further benefit of eliminating the need for user traffic. We can also alter the trafficControl options to reduce the time and number of iterations required.\nYou can apply an optimized Experiment using:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/bootstrap-productpage.yaml You can verify that the Experiment has been created and finishes quickly:\nkubectl -n $NAMESPACE get experiment productpage-bootstrap NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-bootstrap Completed ExperimentSucceeded: Last Iteration Was Completed productpage-v1 0 productpage-v1 100 You can also verify that a virtual service has been created:\nkubectl -n $NAMESPACE get virtualservice NAME GATEWAYS HOSTS AGE productpage.example.com.iter8-experiment [productpage-service] [productpage.example.com] 20m This approach may seem unintuitive. However, we illustrate it here because this bootstrapping issue often arises when automating the use of canary rollouts.\nStep 3: Create an iter8 Canary Experiment We can now create a canary Experiment from version productpage-v1 to productpage-v2. The following command will create the Experiment:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/canary_productpage-v1_to_productpage-v2.yaml You can verify that the Experiment has been created:\nkubectl -n $NAMESPACE get experiment productpage-v2-rollout NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-v2-rollout Pause TargetsNotFound: Missing Candidate productpage-v1 100 productpage-v2 0 The experiment is paused since only the baseline version can be identified. When the candidate version is detected, the experiment will automatically begin execution.\nStep 4: Generate load As in earlier tutorials, emulate requests coming from users using curl:\nwatch -x -n 0.1 curl -Is -H \u0026#39;Host: productpage.example.com\u0026#39; \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; Step 5: Deploy the candidate version productpage-v2 To start the rollout of the new version of the productpage application, deploy the new version:\nkubectl -n $NAMESPACE apply -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/productpage-v2.yaml -f https://raw.githubusercontent.com/iter8-tools/iter8-controller/v0.2.1/doc/tutorials/istio/bookinfo/service/productpage-v2.yaml You can verify the experiment has started:\nkubectl -n $NAMESPACE get experiment productpage-v2-rollout NAME PHASE STATUS BASELINE PERCENTAGE CANDIDATE PERCENTAGE productpage-v2-rollout Progressing IterationUpdate: Iteration 1 Started productpage-v1 80 productpage-v2 20 You can also verify that the VirtualService created by the bootstrap step has been reused:\nkubectl -n $NAMESPACE get virtualservice NAME GATEWAYS HOSTS AGE productpage.example.com.iter8-experiment [productpage-service] [productpage.example.com] 20m As the canary rollout progresses, you should see traffic shift from the baseline to the candidate version until all of the traffic is being sent to the new version.\nCleanup You can cleanup by deleting the namespace:\nkubectl delete ns $NAMESPACE "},{"uri":"https://iter8.tools/archive/v0.2.1/introduction/installation/","title":"Installation","tags":[],"description":"","content":"Installation  Iter8 on Kubernetes and Istio  Learn how to install iter8 on Kubernetes and Istio\n   Iter8 on Red Hat OpenShift  Learn how to install iter8 on Red Hat OpenShift\n   "},{"uri":"https://iter8.tools/archive/v0.2.1/reference/experiment/","title":"Iter8&#39;s experiment CRD","tags":[],"description":"","content":"When iter8 is installed, a new Kubernetes CRD is added to your cluster. Our CRD kind and current API version are as follows:\napiVersion: iter8.tools/v1alpha1 kind: Experiment Below we document iter8\u0026rsquo;s Experiment CRD. For clarity, we break the documentation down into the CRD\u0026rsquo;s 4 sections: spec, action, metrics, and status.\nExperiment spec Following the Kubernetes model, the spec section specifies the details of the object and its desired state. The spec of an Experiment custom resource identifies the target service of a candidate release or A/B test, the baseline deployment corresponding to the stable service version, the candidate deployment corresponding to the service version being assessed, etc. In the YAML representation below, we show sample values for the spec attributes and comments describing their meaning and whether or not they are optional.\nspec: # targetService specifies the reference to experiment targets targetService: # kind of baseline/candidate # options: Service, Deployment # default is Deployment kind: Deployment # name of a kubernetes service which receives actual traffic to the application # it\u0026#39;s required when baseline/candidate are specified as deployments name: reviews # hosts specifies how baseline/candidate can be accessed from outside the cluster # Each entry contains the name of a host and the gateway(istio) associated with it # This is optional and only applies to services directly accessible from outside the K8s cluster hosts: - name: reviews.com gateway: bookinfo-gateway # Name of the baseline and candidate versions (required) # If kind (above) is Deployment, baseline and candidate are references to K8s deployments # If kind (above) is Service, baseline and candidate are references to K8s services baseline: reviews-v3 candidate: reviews-v5 # port of the kubernetes service(.spec.targetService.name) that receives traffic # When there is only one port listening on the service, this is optional # If baseline/candidate are services, they should share the same port number port: 9080 # analysis contains the parameters for configuring the analytics service analysis: # analyticsService specifies analytics service endpoint (optional) # default value is http://iter8-analytics.iter8:8080 analyticsService: http://iter8-analytics.iter8:8080 # endpoint to Grafana dashboard (optional) # default is http://localhost:3000 grafanaEndpoint: http://localhost:3000 # successCriteria is a list of criteria for assessing the candidate version (optional) # if the list is empty, the controller will not rely on the analytics service successCriteria: # metricName: name of the metric to which this criterion applies (required) # the name should match the name of an iter8 metric or that of a user-defined custom metric # names of metrics supported by iter8 out of the box: # iter8_latency: mean latency of the service # iter8_error_rate: mean error rate (~5** HTTP Status codes) of the service # iter8_error_count: total error count (~5** HTTP Status codes) of the service - metricName: iter8_latency # minimum number of data points required to make a decision based on this criterion (optional) # default is 10 # Used by the check_and_increment and epsilon_greedy algorithms # Ignored by other algorithms sampleSize: 100 # the metric value for the candidate version defining this success criterion (required) # it can be an absolute threshold or one relative to the baseline version, depending on the # attribute toleranceType described next tolerance: 0.2 # indicates if the tolerance value above should be interpreted as an absolute threshold or # a threshold relative to the baseline (required) # options: # threshold: the metric value for the candidate must be below the tolerance value above # delta: the tolerance value above indicates the percentage within which the candidate metric value can deviate # from the baseline metric value toleranceType: threshold # The range of possible metric values (optional) # Used by bayesian routing algorithms if available. # Ignored by other algorithms. min_max: # The minimum possible value for the metric min: 0.0 # The maximum possible value for the metric max: 1.0 # indicates whether or not the experiment must finish if this criterion is not satisfied (optional) # default is false stopOnFailure: false # reward is an optional field that can be used when an a/b testing is conducted # When both versions satisfy all the success criteria, the one with higher reward value wins the comparison # This is effective when a bayesian routing strategy is specified in trafficControl (posterior_bayesian_routing or optimistic_bayesian_routing) reward: # the metric whose value is treated as reward (required) - metricName: iter8_latency # The range of possible metric values (optional) min_max: # The minimum possible value for the metric min: 0.0 # The maximum possible value for the metric max: 1.0 # trafficControl controls the experiment durarion and how the controller should change the traffic split trafficControl: # frequency with which the controller calls the analytics service # it corresponds to the duration of each \u0026#34;iteration\u0026#34; of the experiment interval: 30s # maximum number of iterations for this experiment (optional) # the duration of an experiment is defined by maxIterations * internal # default is 100 maxIterations: 6 # the maximum traffic percentage to send to the candidate during an experiment (optional) # default is 50 maxTrafficPercentage: 80 # strategy used to analyze the candidate and shift the traffic (optional) # except for the strategy increment_without_check, the analytics service is called # at each iteration and responds with the appropriate traffic split which the controller honors # options: # check_and_increment # epsilon_greedy # posterior_bayesian_routing # optimistic_bayesian_routing # increment_without_check: increase traffic to candidate by trafficStepSize at each iteration without calling analytics # default is check_and_increment strategy: check_and_increment # the maximum traffic increment per iteration (optional) # default is 2.0 # Used by check_and_increment algorithm # Ignored by other algorithms trafficStepSize: 20 # The required confidence in the recommended traffic split (optional) # default is 0.95 # Used by bayesian routing algorithms # Ignored by other algorithms confidence: 0.9 # determines how the traffic must be split at the end of the experiment (optional) # options: # baseline: all traffic goes to the baseline version # candidate: all traffic goes to the candidate version # both: traffic is split across baseline and candidate # default is candidate onSuccess: candidate # indicates whether or not iter8 should perform a clean-up action at the end of the experiment (optional) # if no action is specified, nothing is done to clean up at the end # if used, the currently supported actions are: # delete: at the end of the experiment, the version that ends up with no traffic (if any) is deleted cleanup: Experiment action: user-provided action The user can interfere with an ongoing experiment by setting the value of an action attribute. Iter8 currently supports 4 user actions: pause, resume, override_success, and override_failure. Pause and resume are self-explanatory. The action override_success causes the experiment to immediately terminate with a success status, whereas override_failure causes the experiment to terminate with a failure status.\n# user-provided input (optional) # options: # pause: pause the experiment # resume: resume a paused experiment # override_success: terminate the experiment indicating that the candidate succeeded # override_failure: abort the experiment indicating that the candidate failed action: \u0026#34;\u0026#34; Experiment metrics Information about all Prometheus metrics known to iter8 are stored in a Kubernetes ConfigMap named iter8config-metrics. When iter8 is installed, that ConfigMap is populated with information on the 3 metrics that iter8 supports out of the box, namely: iter8_latency, iter8_error_rate, and iter8_error_count. Users can add their own custom metrics.\nWhen an Experiment custom resource is created, the iter8 controller will check the metric names referenced by .spec.analysis.successCriteria, look them up in the ConfigMap, retrieve the information about them from the ConfigMap, and store that information in the metrics section of the newly created Experiment object. The information about a metric allows the iter8 analytics service to query Prometheus to retrieve metric values for the baseline version and candidate versions of the service . Below we show an example of how a metric is stored in an Experiment object.\nmetrics: iter8_latency: absent_value: None is_counter: false query_template: (sum(increase(istio_request_duration_seconds_sum{job=\u0026#39;istio-mesh\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels)) / (sum(increase(istio_request_duration_seconds_count{job=\u0026#39;istio-mesh\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels)) sample_size_template: sum(increase(istio_requests_total{job=\u0026#39;istio-mesh\u0026#39;,reporter=\u0026#39;source\u0026#39;}[$interval]$offset_str)) by ($entity_labels) Experiment status Following the Kubernetes model, the status section contains all relevant runtime details pertaining to the Experiment custom resource. In the YAML representation below, we show sample values for the status attributes and comments describing their meaning.\nstatus: # the last analysis state analysisState: {} # assessment returned from the analytics service assessment: conclusions: - The experiment needs to be aborted - All success criteria were not met # list of boolean conditions describing the status of the experiment # for each condition, if the status is \u0026#34;False\u0026#34;, the reason field will give detailed explanations # lastTransitionTime records the time when the last change happened to the corresponding condition # when a condition is not set, its status will be \u0026#34;Unknown\u0026#34; conditions: # AnalyticsServiceNormal is \u0026#34;True\u0026#34; when the controller can get an interpretable response from the analytics service - lastTransitionTime: \u0026#34;2019-12-20T05:38:37Z\u0026#34; status: \u0026#34;True\u0026#34; type: AnalyticsServiceNormal # ExperimentCompleted tells whether the experiment is completed or not - lastTransitionTime: \u0026#34;2019-12-20T05:39:37Z\u0026#34; status: \u0026#34;True\u0026#34; type: ExperimentCompleted # ExperimentSucceeded indicates whether the experiment succeeded or not when it is completed - lastTransitionTime: \u0026#34;2019-12-20T05:39:37Z\u0026#34; message: Aborted reason: ExperimentFailed status: \u0026#34;False\u0026#34; type: ExperimentSucceeded # MetricsSynced states whether the referenced metrics have been retrieved from the ConfigMap and stored in the metrics section - lastTransitionTime: \u0026#34;2019-12-20T05:38:22Z\u0026#34; status: \u0026#34;True\u0026#34; type: MetricsSynced # Ready records the status of the latest-updated condition - lastTransitionTime: \u0026#34;2019-12-20T05:39:37Z\u0026#34; message: Aborted reason: ExperimentFailed status: \u0026#34;False\u0026#34; type: Ready # RoutingRulesReady indicates whether the routing rules are successfully created/updated - lastTransitionTime: \u0026#34;2019-12-20T05:38:22Z\u0026#34; tatus: \u0026#34;True\u0026#34; type: RoutingRulesReady # TargetsProvided is \u0026#34;True\u0026#34; when both the baseline and the candidate versions of the targetService are detected by the controller; otherwise, missing elements will be shown in the reason field - lastTransitionTime: \u0026#34;2019-12-20T05:38:37Z\u0026#34; status: \u0026#34;True\u0026#34; type: TargetsProvided # the current experiment\u0026#39;s iteration currentIteration: 2 # Unix timestamp in nanoseconds when the experiment is created createTimestamp: 1576820317351000 # Unix timestamp in nanoseconds when the experiment started startTimestamp: 1576820317351000 # Unix timestamp in nanoseconds when the experiment finished endTimestamp: 1576820377696000 # The url to he Grafana dashboard pertaining to this experiment grafanaURL: http://localhost:3000/d/eXPEaNnZz/iter8-application-metrics?var-namespace=bookinfo-iter8\u0026amp;var-service=reviews\u0026amp;var-baseline=reviews-v3\u0026amp;var-candidate=reviews-v5\u0026amp;from=1576820317351\u0026amp;to=1576820377696 # the time when the previous iteration was completed lastIncrementTime: \u0026#34;2019-12-20T05:39:07Z\u0026#34; # this is the message to be shown in the STATUS column for the `kubectl` printer, which summarizes the experiment situation message: \u0026#39;ExperimentFailed: Aborted\u0026#39; # the experiment\u0026#39;s current phase # values could be: Progressing, Pause, Completed phase: Completed # the current traffic split trafficSplitPercentage: baseline: 100 candidate: 0 "},{"uri":"https://iter8.tools/archive/v0.2.1/reference/algorithms/","title":"Iter8&#39;s algorithms","tags":[],"description":"","content":"This documentation briefly describes the algorithms supported by iter8 to make decisions during canary releases or A/B testing. These algorithms are part of iter8\u0026rsquo;s analytics service (iter8-analytics) and exposed via REST API. Iter8\u0026rsquo;s Kubernetes controller (iter8-controller) calls the appropriate REST API based on the .spec.trafficControl.strategy set in a custom Experiment resource. Iter8\u0026rsquo;s Experiment CRD is documented here.\nIter8\u0026rsquo;s algorithms are statistically robust. Below, we list the algorithms currently available to users. This list will grow as we introduce other sophisticated algorithms for decision making.\n1. Progressive check-and-increment algorithm (check_and_increment) Input parameters interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) trafficStepSize: # (percentage; e.g., 5) maxTrafficPercentage: # (percentage; e.g., 90) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) This algorithm is suitable for the gradual rollout of a candidate (\u0026ldquo;canary\u0026rdquo;) version. The goal of this strategy is to gradually shift traffic from a baseline (stable) version to a candidate version, as long as the candidate version continues to pass the success criteria defined by the user.\nWhen the experiment begins, the traffic split is as follows: trafficStepSize% to the candidate version, and 100 - trafficStepSize% to the baseline version. At the end of each iteration (whose duration is determined by the interval parameter), iter8 checks if there are enough data points to decide whether the candidate version satisfies the success criteria (i.e, whether enough requests were sent to make a statistically robust assessment). If there is enough data to make a decision, and the candidate version satisfies all criteria, iter8 increases the traffic to the candidate version by trafficStepSize. Else, if there is insufficient data, or if the candidate version fails to satisfy one or more success criteria, then the traffic split does not change. Furthermore, if a failing criterion has been declared by the user as critical, iter8 aborts the experiment and makes sure all traffic goes to the baseline version. This is a rollback situation.\nA successful experiment will last for a duration of length interval * maxIterations. In case of success, the user can specify whether iter8 should: (1) send all traffic to the candidate; (2) roll back to the baseline despite success; or (3) split traffic across both versions. If the traffic is to be split across both versions, then the final split will be as follows: maxTrafficPercentage% to the candidate and 1 - maxTrafficPercentage to the baseline.\n2. Decaying epsilon-greedy algorithm (epsilon_greedy) interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) maxTrafficPercentage: # (percentage; e.g., 90) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) This algorithm can be applied to canary releases as well as A/B or A/B/n testing. The goal of this strategy is to explore two or more competing versions, aiming to maximize a reward (which is typically associated with business-oriented metrics) while making sure that the defined success criteria (typically associated with performance-oriented and/or correctness-oriented metrics) are satisfied.\nUnlike the check-and-increment strategy described above, this algorithm automatically decides what the proper traffic split should be at the end of each iteration and does not require the user to supply a value for the traffic increment per iteration. It converges relatively quickly to the \u0026ldquo;optimal\u0026rdquo; version as more iterations occur over time.\nIn A/B or A/B/n testing, the \u0026ldquo;optimality\u0026rdquo; of a version relates to maximizing the reward during the course of an experiment while satisfying the success criteria. In the context of canary releases, an implicit reward metric is used to indicate whether or not the success criteria are satisfied at each iteration.\n3. Posterior Bayesian Routing (PBR) (posterior_bayesian_routing) interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) maxTrafficPercentage: # (percentage; e.g., 90) confidence: # (float; e.g, 0.95) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) This algorithm provides a robust way of shifting application traffic to the best version using the principles of Bayesian statistics. Like the decaying epsilon-greedy strategy described above, PBR can be applied to canary releases as well as A/B or A/B/n testing scenarios. The goal of this strategy is to shift traffic to the optimal version subject to the user-defined success criteria.\nIn this algorithm, the metrics used in success criteria are associated with Bayesian belief distributions, specifically, whose parameters are learnt from metric observations through the course of the experiment. At each iteration, the algorithm computes the probability of the candidate being the \u0026ldquo;best\u0026rdquo; version (i.e., satisfying all the success criteria) and the probability of the baseline being the \u0026ldquo;best\u0026rdquo; version (i.e, the complementary probability to what is computed above). Traffic is split across these two versions in proportion to their probabilities. At the end of the experiment, if the probability of candidate being the \u0026ldquo;best\u0026rdquo; version exceeds the value of confidence parameter, then, the experiment is declared a success and a roll-forward to candidate occurs.\n4. Optimistic Bayesian Routing (OBR) (optimistic_bayesian_routing) interval: # (time; e.g., 30s) maxIterations: # (integer; e.g., 1000) maxTrafficPercentage: # 80 (percentage; e.g., 90) confidence: # (float; e.g, 0.95) onSuccess: # (string enum; possible values are: \u0026#34;candidate\u0026#34;, \u0026#34;baseline\u0026#34;, \u0026#34;both\u0026#34;) Optimistic Bayesian Routing is a variation of PBR, sharing the same goal of shifting traffic to the optimal version in a statistically robust manner. The main difference between OBR and PBR lies in the way values are sampled from the distributions for reward and feasibility constraints: this algorithm has a more optimistic approach and tends to exhibit a faster convergence rate.\n"},{"uri":"https://iter8.tools/archive/v0.2.1/releases/","title":"Older releases","tags":[],"description":"","content":" v0.2.1 v0.2.0 v0.1.1 v0.1.0 v0.0.1  "},{"uri":"https://iter8.tools/archive/v0.2.1/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://iter8.tools/archive/v0.2.1/tags/","title":"Tags","tags":[],"description":"","content":""}]